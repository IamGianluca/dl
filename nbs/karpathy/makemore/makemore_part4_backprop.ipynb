{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b4411a6",
   "metadata": {},
   "source": [
    "# Building `makemore` Part 4: Becoming a Backprop Ninja\n",
    "\n",
    "\n",
    "Lecture: [YouTube](https://youtu.be/q8SA3rM6ckI)\n",
    "\n",
    "We are going to manually reimplement what `loss.backward()` does. In this way, we can better understand how gradients flow in the backward pass and get intuition that will prevent us from committing silly mistakes when building a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58016936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30aad802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"./names.txt\", \"r\").read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e214b44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3951e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vacabulary of characters and mapping to/from integers\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "s2i = {s: i + 1 for i, s in enumerate(chars)}\n",
    "s2i[\".\"] = 0\n",
    "i2s = {i: s for s, i in s2i.items()}\n",
    "vocab_size = len(i2s)\n",
    "print(i2s)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6055251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = (\n",
    "    3  # context length: how many characters do we take to predict the next one?\n",
    ")\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    x, y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = s2i[ch]\n",
    "            x.append(context)\n",
    "            y.append(ix)\n",
    "            context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "    x = torch.tensor(x)\n",
    "    y = torch.tensor(y)\n",
    "    print(x.shape, y.shape)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "x_trn, y_trn = build_dataset(words[:n1])  # 80%\n",
    "x_val, y_val = build_dataset(words[n1:n2])  # 10%\n",
    "x_tst, y_tst = build_dataset(words[n2:])  # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc8f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(\n",
    "        f\"{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "844eb78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "# layer 1\n",
    "W1 = (\n",
    "    torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "    * (5 / 3)\n",
    "    / ((n_embd * block_size) ** 0.5)\n",
    ")\n",
    "b1 = (\n",
    "    torch.randn(n_hidden, generator=g) * 0.1\n",
    ")  # using b1 just for fun, it's useless because of BN\n",
    "# layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# batchnorm parameters\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "# note: i am initializating many of these parameters (e.g., biases) in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))  # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72241444",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size  # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, x_trn.shape[0], (batch_size,), generator=g)\n",
    "xb, yb = x_trn[ix], y_trn[ix]  # batch X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ebf67",
   "metadata": {},
   "source": [
    "Below is the forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30166f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3353, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[xb]  # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "# linear layer 1\n",
    "hprebn = embcat @ W1 + b1  # hidden layer pre-activation\n",
    "# batchnorm layer\n",
    "bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = (\n",
    "    1 / (n - 1) * (bndiff2).sum(0, keepdim=True)\n",
    ")  # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# non-linearity\n",
    "h = torch.tanh(hpreact)  # hidden layer\n",
    "# linear layer 2\n",
    "logits = h @ W2 + b2  # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = (\n",
    "    counts_sum**-1\n",
    ")  # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [\n",
    "    logprobs,\n",
    "    probs,\n",
    "    counts,\n",
    "    counts_sum,\n",
    "    counts_sum_inv,  # afaik there is no cleaner way\n",
    "    norm_logits,\n",
    "    logit_maxes,\n",
    "    logits,\n",
    "    h,\n",
    "    hpreact,\n",
    "    bnraw,\n",
    "    bnvar_inv,\n",
    "    bnvar,\n",
    "    bndiff2,\n",
    "    bndiff,\n",
    "    hprebn,\n",
    "    bnmeani,\n",
    "    embcat,\n",
    "    emb,\n",
    "]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d0dc4",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Backprop through the whole thing manually, backpropagating through exactly all of the variables as they are defined in the forward pass above, one by one.\n",
    "\n",
    "A few notes before we start:\n",
    "* As a naming convention, we are going to name each variable storing the partial derivative of the loss w.r.t. each parameter group, `d` + `<parameter group>`. For instance, the partial derivative of the loss w.r.t. `logprobs`, $ \\frac{\\partial J(w, b)}{\\partial \\text{logprobs}} $, will be named `dlogprobs`.\n",
    "* We are also omitting $ \\frac{\\partial J(w, b)}{\\partial J(w, b} $ as that is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc0512e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bnbias          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "bnvar_inv       | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
      "bndiff          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
      "bnmeani         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "hprebn          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
      "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
      "W1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "b1              | exact: False | approximate: True  | maxdiff: 3.958120942115784e-09\n",
      "emb             | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
      "C               | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n"
     ]
    }
   ],
   "source": [
    "# create a matrix of all zero of the same shape as the `logprobs` array, to store the \n",
    "# gradients `dlogprobs` \n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "# update the gradient only of the elements corresponding to the ground truth predictions \n",
    "# with the partial derivative of the loss w.r.t. logprobs\n",
    "dlogprobs[range(n), yb] = -1.0 / n\n",
    "cmp(\"logprobs\", dlogprobs, logprobs)\n",
    "\n",
    "dprobs = (1.0 / probs) * dlogprobs # multiply by dlogprobs as we are using the chain rule\n",
    "cmp(\"probs\", dprobs, probs)\n",
    "\n",
    "# in this case, we need to keep in mind that `counts`, `dprobs`, and `count_sum_inv` have\n",
    "# different shapes:\n",
    "# >>> counts.shape, dprobs.shape, counts_sum_inv.shape\n",
    "# (torch.Size([32, 27]), torch.Size([32, 27])), torch.Size([32, 1])) \n",
    "# we need `dcounts_sum_inv` to be of shape (32, 1), thus accumulating the gradients of \n",
    "# each row\n",
    "# d11 d12 d13     b1 (= d11 + d12 + d13)\n",
    "# d21 d22 d23 --> b2 (= d21 + d22 + d23)\n",
    "# d31 d32 d33     b3 (= d31 + d32 + d33)\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True)\n",
    "cmp(\"counts_sum_inv\", dcounts_sum_inv, counts_sum_inv)\n",
    "\n",
    "# `dcounts` is a trickier example, as `counts` is used in two places in the computational\n",
    "# graph:\n",
    "# 1. probs = counts * counts_sum_inv, AND...\n",
    "# 2. counts_sum = counts.sum(1, keepdims=True)\n",
    "# we are going to work initially on the first contribution, and later add the second part \n",
    "# to it to compute the total gradient\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "\n",
    "dcounts_sum = (-counts_sum ** -2) * dcounts_sum_inv\n",
    "cmp(\"counts_sum\", dcounts_sum, counts_sum)\n",
    "\n",
    "# let's now resume working on `dcounts` by processing the second contribution:\n",
    "# `counts_sum = counts.sum(1, keepdims=True)`\n",
    "# also in this case, we need to keep an eye on the dimension of the variables:\n",
    "# >>> counts.shape, dcounts_sum.shape\n",
    "# (torch.Size([32, 27]), torch.Size([32, 1]))\n",
    "# what we want to accomplish is to add to all elements in `dcounts` the `dcounts_sum`\n",
    "# contribution. since `dcounts_sum`\n",
    "# a11 a12 a13   b1 --> a11 a12 a13   b1 b1 b1    \n",
    "# a21 a22 a23 + b2 --> a21 a22 a23 + b2 b2 b2  \n",
    "# a31 a32 a33   b3 --> a31 a32 a33   b3 b3 b3    \n",
    "# this is a simple broadcasting operation, where `dcounts_sum` expands from (32, 1) \n",
    "# to (32, 27)\n",
    "# NOTE: karpathy implements it in a slightly move convoluted way:\n",
    "# >>> dcounts += torch.ones_like(counts) * dcounts_sum \n",
    "dcounts += dcounts_sum # the += operation is because we are summing the contribution of both branches\n",
    "cmp('counts', dcounts, counts)\n",
    "# NOTE: the max() operation fans out the gradient to all elements of the activations that were\n",
    "# included in the operation equally! >>> risk of accidental gradient explosion??\n",
    "\n",
    "dnorm_logits = counts * dcounts  # counts = norm_logits.exp(); in this way we save some FLOPs :-)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "\n",
    "# `logits` is used in two places in the computational graph:\n",
    "# >>> logit_maxes = logits.max(1, keepdim=True).values\n",
    "# >>> norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "# the shape of `logits` and `logit_maxes` is not the same, as there is an implicit \n",
    "# broadcasting operation in the `logits - logit_maxes` operation:\n",
    "# >>> logits.shape, logit_maxes.shape\n",
    "# (torch.Size([32, 27]), torch.Size([32, 1]))\n",
    "# `logit_maxes` is broadcasted from (32, 1) to (32, 27) \n",
    "# c11 c12 c13   a11 a12 a13   c11\n",
    "# c21 c22 c23 = a21 a22 a23 - c21\n",
    "# c31 c32 c33   a31 a32 a33   c31\n",
    "dlogits = dnorm_logits.clone()  # 1st contribution\n",
    "\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdims=True)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "if not torch.allclose(dlogit_maxes, torch.zeros_like(dlogit_maxes)):\n",
    "    # NOTE: `dlogit_maxes` should be very close to 0, because that offset we use to normalize the logit vector before passing it to torch.exp()\n",
    "    # should have no impact on the loss! In fact, we could have used any constant to normalize the logit vector without impacting the value of\n",
    "    # loss. The only reason why we are using max() is to guarantee that the max value of the \"normalized\" logit vector before passing it to \n",
    "    # torch.exp() is zero, to avoid any potential overflow.\n",
    "    print(\"Error: some elements of `dlogit_maxes` are not close to zero!\")\n",
    "\n",
    "# here we want the derivative to flow through where those maximum values occurred in the logit vector\n",
    "# NOTE: karphathy implements it with a different syntax, but the outcome is the same:\n",
    "# >>> dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dlogits[range(n), logits.max(1).indices] += dlogit_maxes.view(-1, )  # 2nd contribution\n",
    "cmp('logits', dlogits, logits)\n",
    "\n",
    "# check notes in reMarkable for the full manual derivation\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "\n",
    "# thanks to google search:\n",
    "# a = tanh(x)\n",
    "# d/dx tanh(x) = 1-a**2\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "\n",
    "# forward pass: hpreact = bngain * bnraw + bnbias\n",
    "# here we have an element-wise multiplication & broadcasting\n",
    "# >>> bngain.shape, bnraw.shape, bnbias.shape\n",
    "# (torch.Size([1, 64]), torch.Size([32, 64]), torch.Size([1, 64]))\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "\n",
    "# forward pass: bnraw = bndiff * bnvar_inv\n",
    "dbndiff = bnvar_inv * dbnraw  # 1st contribution\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "\n",
    "# forward pass: bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "dbnvar = (-0.5 * (bnvar + 1e-5) ** -1.5) * dbnvar_inv\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "\n",
    "# forward pass: bnvar = (\n",
    "#                   1 / (n - 1) * (bndiff2).sum(0, keepdim=True)\n",
    "#               )  # note: Bessel's correction (dividing by n-1, not n)\n",
    "# Note: karpathy writes it in an equivalent, but different, way: dbndiff2 = (1.0/(n - 1.0)) *  torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff2 = (1.0 / (n - 1.0)) * dbnvar\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "\n",
    "# forward pass: bndiff2 = bndiff**2\n",
    "dbndiff += 2 * bndiff * dbndiff2  # 2nd contribution\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "\n",
    "# forward pass: bndiff = hprebn - bnmeani\n",
    "dbnmeani = (-dbndiff).sum(0, keepdim=True)\n",
    "dhprebn = dbndiff.clone()  # 1st contribution\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "\n",
    "# forward pass: bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "# Note: karpathy writes it in the equivalent expression: dhprebn += (1.0 / n) * torch.ones_like(hprebn) * dbnmeani \n",
    "#  We prefer to keep the broadcasting operation implicit.\n",
    "dhprebn += (1.0 / n) * dbnmeani  # 2nd contribution\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "\n",
    "# forward pass: hprebn = embcat @ W1 + b1  # hidden layer pre-activation\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "\n",
    "# forward pass: embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "demb = dembcat.view(emb.shape)\n",
    "cmp('emb', demb, emb)\n",
    "\n",
    "# forward pass: emb = C[xb]  # embed the characters into vectors\n",
    "# print(emb.shape, C.shape, xb.shape)\n",
    "# print(xb[:5])\n",
    "# torch.Size([32, 3, 10]) torch.Size([27, 10]) torch.Size([32, 3])\n",
    "# tensor([[ 1,  1,  4],\n",
    "#         [18, 14,  1],\n",
    "#         [11,  5,  9],\n",
    "#         [ 0,  0,  1],\n",
    "#         [12, 15, 14]])\n",
    "# emb[bs, cntx_len, emb_sz]: \n",
    "# C[n_chars, emb_sz]: embedding matrix\n",
    "# xb[bs, char_idx]:\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(xb.shape[0]): # bs\n",
    "    for j in range(xb.shape[1]): # char_idx\n",
    "        ix = xb[k, j]\n",
    "        dC[ix] += demb[k, j, :]\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc712218",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. Always check the size of the tensors. The parameter group and the derivative of the loss w.r.t. that parameter group must have the same shape. At times we will need to undo a broadcasting operation or aggregating operation (e.g., sum or average over certain axes). \n",
    "1. If a parameter group appears in multiple branches of the computational graph, we must accumulate the gradient of both branches.\n",
    "\n",
    "**TODO**: Reduce `maxdiff` of `hpreact`. According to some prelimanary reseach in YouTube and GitHub, this seems to be caused by the PyTorch version being used. Is there a bug in PyTorch? Our math matches what Karpathy's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c0b00",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Backprob through `cross_entropy` but all in one go. To complete this challenge look at the mathematical expression of the loss, take the derivative, simplify the expression, and just write it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fa6cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.33529949 (diff: 0.0)\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = (\n",
    "#     counts_sum**-1\n",
    "# )  # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, yb)\n",
    "print(f\"Loss: {loss_fast.item():.8f} (diff: {(loss_fast - loss).item()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670d638",
   "metadata": {},
   "source": [
    "The formula for the cross-entropy loss is: \n",
    "\n",
    "$ \\text{CE} = \\sum_{U} \\sum_{I} y_{i, u} \\log (p_{i, u}) $\n",
    "\n",
    "One extra thing to note is that in the formula above, $ p_{i, u} $ are probabilities, but `F.cross_entropy` expects the `input` to be unnormalized logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "521ccbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msize_average\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msize_average\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"Compute the cross entropy loss between input logits and target.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    See :class:`~torch.nn.CrossEntropyLoss` for details.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        input (Tensor) : Predicted unnormalized logits;\u001b[0m\n",
       "\u001b[0;34m            see Shape section below for supported shapes.\u001b[0m\n",
       "\u001b[0;34m        target (Tensor) : Ground truth class indices or class probabilities;\u001b[0m\n",
       "\u001b[0;34m            see Shape section below for supported shapes.\u001b[0m\n",
       "\u001b[0;34m        weight (Tensor, optional): a manual rescaling weight given to each\u001b[0m\n",
       "\u001b[0;34m            class. If given, has to be a Tensor of size `C`\u001b[0m\n",
       "\u001b[0;34m        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\u001b[0m\n",
       "\u001b[0;34m            the losses are averaged over each loss element in the batch. Note that for\u001b[0m\n",
       "\u001b[0;34m            some losses, there multiple elements per sample. If the field :attr:`size_average`\u001b[0m\n",
       "\u001b[0;34m            is set to ``False``, the losses are instead summed for each minibatch. Ignored\u001b[0m\n",
       "\u001b[0;34m            when reduce is ``False``. Default: ``True``\u001b[0m\n",
       "\u001b[0;34m        ignore_index (int, optional): Specifies a target value that is ignored\u001b[0m\n",
       "\u001b[0;34m            and does not contribute to the input gradient. When :attr:`size_average` is\u001b[0m\n",
       "\u001b[0;34m            ``True``, the loss is averaged over non-ignored targets. Note that\u001b[0m\n",
       "\u001b[0;34m            :attr:`ignore_index` is only applicable when the target contains class indices.\u001b[0m\n",
       "\u001b[0;34m            Default: -100\u001b[0m\n",
       "\u001b[0;34m        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\u001b[0m\n",
       "\u001b[0;34m            losses are averaged or summed over observations for each minibatch depending\u001b[0m\n",
       "\u001b[0;34m            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\u001b[0m\n",
       "\u001b[0;34m            batch element instead and ignores :attr:`size_average`. Default: ``True``\u001b[0m\n",
       "\u001b[0;34m        reduction (str, optional): Specifies the reduction to apply to the output:\u001b[0m\n",
       "\u001b[0;34m            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\u001b[0m\n",
       "\u001b[0;34m            ``'mean'``: the sum of the output will be divided by the number of\u001b[0m\n",
       "\u001b[0;34m            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\u001b[0m\n",
       "\u001b[0;34m            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\u001b[0m\n",
       "\u001b[0;34m            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\u001b[0m\n",
       "\u001b[0;34m        label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\u001b[0m\n",
       "\u001b[0;34m            of smoothing when computing the loss, where 0.0 means no smoothing. The targets\u001b[0m\n",
       "\u001b[0;34m            become a mixture of the original ground truth and a uniform distribution as described in\u001b[0m\n",
       "\u001b[0;34m            `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Shape:\u001b[0m\n",
       "\u001b[0;34m        - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\u001b[0m\n",
       "\u001b[0;34m          in the case of `K`-dimensional loss.\u001b[0m\n",
       "\u001b[0;34m        - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\u001b[0m\n",
       "\u001b[0;34m          :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\u001b[0m\n",
       "\u001b[0;34m          If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        where:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. math::\u001b[0m\n",
       "\u001b[0;34m            \\begin{aligned}\u001b[0m\n",
       "\u001b[0;34m                C ={} & \\text{number of classes} \\\\\u001b[0m\n",
       "\u001b[0;34m                N ={} & \\text{batch size} \\\\\u001b[0m\n",
       "\u001b[0;34m            \\end{aligned}\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Examples::\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        >>> # Example of target with class indices\u001b[0m\n",
       "\u001b[0;34m        >>> input = torch.randn(3, 5, requires_grad=True)\u001b[0m\n",
       "\u001b[0;34m        >>> target = torch.randint(5, (3,), dtype=torch.int64)\u001b[0m\n",
       "\u001b[0;34m        >>> loss = F.cross_entropy(input, target)\u001b[0m\n",
       "\u001b[0;34m        >>> loss.backward()\u001b[0m\n",
       "\u001b[0;34m        >>>\u001b[0m\n",
       "\u001b[0;34m        >>> # Example of target with class probabilities\u001b[0m\n",
       "\u001b[0;34m        >>> input = torch.randn(3, 5, requires_grad=True)\u001b[0m\n",
       "\u001b[0;34m        >>> target = torch.randn(3, 5).softmax(dim=1)\u001b[0m\n",
       "\u001b[0;34m        >>> loss = F.cross_entropy(input, target)\u001b[0m\n",
       "\u001b[0;34m        >>> loss.backward()\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e89849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward  pass\n",
    "# dlogits = ???\n",
    "# cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952f196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
