{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4c5310",
   "metadata": {},
   "source": [
    "Reference paper: [Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A Neural Probabilistic Language Model. J. Mach. Learn. Res. 3, (February 2003), 1137–1155.](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "\n",
    "One main difference in our implementation is that we are working with characters instead of words. The vocabulary size in the Bengio's paper is 17,000 words, whereas we are going to have a vocabulary of 27 characters (26 characters and the `<.>` special character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c55d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f793b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"./names.txt\", \"r\").read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21dcac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cccabf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vacabulary of characters and mapping to/from integers\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "s2i = {s: i + 1 for i, s in enumerate(chars)}\n",
    "s2i[\".\"] = 0\n",
    "i2s = {i: s for s, i in s2i.items()}\n",
    "vocab_size = len(i2s)\n",
    "print(i2s)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb50c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3  # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "    x, y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = s2i[ch]\n",
    "            x.append(context)\n",
    "            y.append(ix)\n",
    "            context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "    x = torch.tensor(x)\n",
    "    y = torch.tensor(y)\n",
    "    print(x.shape, y.shape)\n",
    "    return x, y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "x_trn, y_trn = build_dataset(words[:n1])    # 80%\n",
    "x_val, y_val = build_dataset(words[n1:n2])  # 10%\n",
    "x_tst, y_tst = build_dataset(words[n2:])    # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c16fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 11897\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn((n_hidden), generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g)\n",
    "b2 = torch.randn((vocab_size), generator=g)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"Number of trainable parameters: {sum(p.nelement() for p in parameters)}\")  # total number of parameters in the model\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79569768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 27.8817\n",
      "  10000/ 200000: 2.9417\n",
      "  20000/ 200000: 2.5795\n",
      "  30000/ 200000: 2.7819\n",
      "  40000/ 200000: 2.0216\n",
      "  50000/ 200000: 2.5858\n",
      "  60000/ 200000: 2.3217\n",
      "  70000/ 200000: 2.1007\n",
      "  80000/ 200000: 2.2893\n",
      "  90000/ 200000: 2.2368\n",
      " 100000/ 200000: 1.9774\n",
      " 110000/ 200000: 2.4507\n",
      " 120000/ 200000: 1.9623\n",
      " 130000/ 200000: 2.3657\n",
      " 140000/ 200000: 2.2424\n",
      " 150000/ 200000: 2.1469\n",
      " 160000/ 200000: 2.2506\n",
      " 170000/ 200000: 1.7543\n",
      " 180000/ 200000: 2.1055\n",
      " 190000/ 200000: 1.8174\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "bs = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, x_trn.shape[0], (bs,), generator=g)\n",
    "    xb, yb = x_trn[ix], y_trn[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[xb]  # embed the characters into vectors\n",
    "    emb_cat = emb.view((emb.shape[0], -1))  # concat the vectors\n",
    "    h_pre_act = emb_cat @ W1 + b1  # hidden layer pre-activation\n",
    "    h = torch.tanh(h_pre_act)  # hidden layer\n",
    "    logits = h @ W2 + b2  # output layer\n",
    "    loss = F.cross_entropy(logits, yb)  # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100_000 else 0.01\n",
    "    with torch.no_grad():\n",
    "        for p in parameters:\n",
    "            p -= lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i % 10_000 == 0:\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0822b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4481ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.126697540283203\n",
      "val 2.171041250228882\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()  # this decorator disable gradient tracking\n",
    "def split_loss(split: str):\n",
    "    x, y = {\n",
    "        \"train\": (x_trn, y_trn),\n",
    "        \"val\": (x_val, y_val),\n",
    "        \"test\": (x_tst, y_tst),\n",
    "    }[split]\n",
    "    emb = C[x]  # (bs, block_size, n_embd)\n",
    "    emb_cat = emb.view((emb.shape[0], -1))  # concat into (bs, block_size * n_embd)\n",
    "    h_pre_act = emb_cat @ W1 + b1  # (bs, n_hidden)\n",
    "    h = torch.tanh(h_pre_act)  # (bs, n_hidden)\n",
    "    logits = h @ W2 + b2  # (bs, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)  \n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d93946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mona.\n",
      "mayah.\n",
      "see.\n",
      "mel.\n",
      "ryllo.\n",
      "emmadiengramira.\n",
      "eredielin.\n",
      "shy.\n",
      "jen.\n",
      "eden.\n",
      "eson.\n",
      "arleiyah.\n",
      "hone.\n",
      "cayshubergihimier.\n",
      "kendreelynn.\n",
      "nochorius.\n",
      "macder.\n",
      "yaque.\n",
      "ehs.\n",
      "kayjahsin.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "block_size = 3\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        # forward pass of the neural network\n",
    "        emb = C[torch.tensor([context])]  # (1, block_size, d)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if the special \".\" token break\n",
    "        if ix == 0:\n",
    "            break\n",
    "            \n",
    "    print(\"\".join(i2s[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c53138",
   "metadata": {},
   "source": [
    "## Debugging: The init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d5a5d",
   "metadata": {},
   "source": [
    "The loss we have after the first forward pass is too high, which is a red flag. This could have been caused by a poor initialization. We would expect an initial loss closer to $ - \\log{\\frac{1}{\\text{vocab. lenght}}} = - \\log{\\frac{1}{27}} $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84ad2397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2958)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.tensor(1/27).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e296eea",
   "metadata": {},
   "source": [
    "To better understand what is happening, let's use an example using 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5f83f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([ 0.6115, -0.7783, -0.2394,  1.3746])\n",
      "loss: 2.1914122104644775\n"
     ]
    }
   ],
   "source": [
    "# 4-d example of the issue\n",
    "logits = torch.randn(4)\n",
    "y_true = torch.tensor([0., 0., 1., 0.])\n",
    "loss = F.cross_entropy(input=logits, target=y_true)\n",
    "print(f\"logits: {logits}\\nloss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06e9b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([  4.3236,   3.2203, -11.5777,  -7.3739])\n",
      "loss: 16.187833786010742\n"
     ]
    }
   ],
   "source": [
    "# 4-d example of the issue\n",
    "logits = torch.randn(4) * 10  # <-- artificially increase the values of the logits \n",
    "y_true = torch.tensor([0., 0., 1., 0.])\n",
    "loss = F.cross_entropy(input=logits, target=y_true)\n",
    "print(f\"logits: {logits}\\nloss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f74eaf",
   "metadata": {},
   "source": [
    "What is happening is that the logits generated by our model are too extreme at the beginning of the training process. This causes the loss to become extremely large in those cases when we are very confidently wrong (e.g., high logit for the wrong class, and low logits for the other classes). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd2459",
   "metadata": {},
   "source": [
    "Let's inspect the logits after the first forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd6e9a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 11897\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn((n_hidden), generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g)\n",
    "b2 = torch.randn((vocab_size), generator=g)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"Number of trainable parameters: {sum(p.nelement() for p in parameters)}\")  # total number of parameters in the model\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f06d367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 24.8074\n",
      "tensor([  9.8607,   4.2133,  -4.9696,  -2.3298, -12.4576,  -7.3652,  -1.7257,\n",
      "          0.4356,   5.5772,  -2.7682,  -7.0788,  -2.7884,  -5.8150,  -4.5269,\n",
      "         11.5294,   7.0656, -35.1134,   7.3326,  15.4572, -11.8989, -17.4142,\n",
      "         13.8119,  -5.6911,  -4.5338, -11.9642,   2.0828,  -1.8851],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "bs = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, x_trn.shape[0], (bs,), generator=g)\n",
    "    xb, yb = x_trn[ix], y_trn[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[xb]  # embed the characters into vectors\n",
    "    emb_cat = emb.view((emb.shape[0], -1))  # concat the vectors\n",
    "    h_pre_act = emb_cat @ W1 + b1  # hidden layer pre-activation\n",
    "    h = torch.tanh(h_pre_act)  # hidden layer\n",
    "    logits = h @ W2 + b2  # output layer\n",
    "    loss = F.cross_entropy(logits, yb)  # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100_000 else 0.01\n",
    "    with torch.no_grad():\n",
    "        for p in parameters:\n",
    "            p -= lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i % 10_000 == 0:\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "        print(logits[0])\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2053a2",
   "metadata": {},
   "source": [
    "To fix this, let's set `b2` to zeroes, and reduce the magnitude of the init values in the `W2` weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8260b09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 11897\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn((n_hidden), generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01  # not zero!\n",
    "b2 = torch.randn((vocab_size), generator=g) * 0  # reset the bias term\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"Number of trainable parameters: {sum(p.nelement() for p in parameters)}\")  # total number of parameters in the model\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "caa025f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3221\n",
      "tensor([-0.0249,  0.3523, -0.1267,  0.0541,  0.1662, -0.1171, -0.0140,  0.0124,\n",
      "         0.0921,  0.1135,  0.1362, -0.0925,  0.0444, -0.2459, -0.0971,  0.0125,\n",
      "        -0.0957, -0.1376, -0.2329, -0.0137,  0.2452,  0.1616,  0.1665, -0.1094,\n",
      "        -0.2154,  0.0725,  0.1141], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "bs = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, x_trn.shape[0], (bs,), generator=g)\n",
    "    xb, yb = x_trn[ix], y_trn[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[xb]  # embed the characters into vectors\n",
    "    emb_cat = emb.view((emb.shape[0], -1))  # concat the vectors\n",
    "    h_pre_act = emb_cat @ W1 + b1  # hidden layer pre-activation\n",
    "    h = torch.tanh(h_pre_act)  # hidden layer\n",
    "    logits = h @ W2 + b2  # output layer\n",
    "    loss = F.cross_entropy(logits, yb)  # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100_000 else 0.01\n",
    "    with torch.no_grad():\n",
    "        for p in parameters:\n",
    "            p -= lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i % 10_000 == 0:\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "        print(logits[0])\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402fe1b",
   "metadata": {},
   "source": [
    "This is much better, as the logits are more uniformly distributed, without extreme values. These are the logits from the very first forward pass, so it would be bad if they are heavily biased towards a particular output, as there hasn't been any learning yet (no backprop.).\n",
    "\n",
    "Let's run the full optimization, not just the first forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "342200a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 11897\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn((n_hidden), generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01  # not zero!\n",
    "b2 = torch.randn((vocab_size), generator=g) * 0  # reset the bias term\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"Number of trainable parameters: {sum(p.nelement() for p in parameters)}\")  # total number of parameters in the model\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "602b7c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3221\n",
      "  10000/ 200000: 2.1900\n",
      "  20000/ 200000: 2.4196\n",
      "  30000/ 200000: 2.6067\n",
      "  40000/ 200000: 2.0601\n",
      "  50000/ 200000: 2.4988\n",
      "  60000/ 200000: 2.3902\n",
      "  70000/ 200000: 2.1344\n",
      "  80000/ 200000: 2.3369\n",
      "  90000/ 200000: 2.1299\n",
      " 100000/ 200000: 1.8329\n",
      " 110000/ 200000: 2.2053\n",
      " 120000/ 200000: 1.8540\n",
      " 130000/ 200000: 2.4566\n",
      " 140000/ 200000: 2.1879\n",
      " 150000/ 200000: 2.1118\n",
      " 160000/ 200000: 1.8956\n",
      " 170000/ 200000: 1.8645\n",
      " 180000/ 200000: 2.0326\n",
      " 190000/ 200000: 1.8417\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "bs = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, x_trn.shape[0], (bs,), generator=g)\n",
    "    xb, yb = x_trn[ix], y_trn[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[xb]  # embed the characters into vectors\n",
    "    emb_cat = emb.view((emb.shape[0], -1))  # concat the vectors\n",
    "    h_pre_act = emb_cat @ W1 + b1  # hidden layer pre-activation\n",
    "    h = torch.tanh(h_pre_act)  # hidden layer\n",
    "    logits = h @ W2 + b2  # output layer\n",
    "    loss = F.cross_entropy(logits, yb)  # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100_000 else 0.01\n",
    "    with torch.no_grad():\n",
    "        for p in parameters:\n",
    "            p -= lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i % 10_000 == 0:\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e7c996a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.069589138031006\n",
      "val 2.1310744285583496\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()  # this decorator disable gradient tracking\n",
    "def split_loss(split: str):\n",
    "    x, y = {\n",
    "        \"train\": (x_trn, y_trn),\n",
    "        \"val\": (x_val, y_val),\n",
    "        \"test\": (x_tst, y_tst),\n",
    "    }[split]\n",
    "    emb = C[x]  # (bs, block_size, n_embd)\n",
    "    emb_cat = emb.view((emb.shape[0], -1))  # concat into (bs, block_size * n_embd)\n",
    "    h_pre_act = emb_cat @ W1 + b1  # (bs, n_hidden)\n",
    "    h = torch.tanh(h_pre_act)  # (bs, n_hidden)\n",
    "    logits = h @ W2 + b2  # (bs, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)  \n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90243fa",
   "metadata": {},
   "source": [
    "This is a significant improvement over the previous model! The reason for that is that we are spending more cycles learning instead of using the first few thousand iteration to squash down the weights to avoid extreme logits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c135f36",
   "metadata": {},
   "source": [
    "## Debugging: The hidden activation states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d9fc4",
   "metadata": {},
   "source": [
    "This time we are going to focus on the values of the hidden layer activations. There are taking on very extreme values, closer to $ -1.0 $ and $ 1.0 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3bad8915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 11897\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn((n_hidden), generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01  # not zero!\n",
    "b2 = torch.randn((vocab_size), generator=g) * 0  # reset the bias term\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"Number of trainable parameters: {sum(p.nelement() for p in parameters)}\")  # total number of parameters in the model\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62d68f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3221\n",
      "tensor([[ 0.8100, -0.8997, -0.9993,  ..., -0.9097, -1.0000,  1.0000],\n",
      "        [-1.0000, -0.9571, -0.7145,  ...,  0.4898,  0.9090,  0.9937],\n",
      "        [ 0.9983, -0.3340,  1.0000,  ...,  0.9443,  0.9905,  1.0000],\n",
      "        ...,\n",
      "        [-1.0000,  0.9604, -0.1418,  ..., -0.1266,  1.0000,  1.0000],\n",
      "        [-1.0000, -0.4385, -0.8882,  ..., -0.3316,  0.9995,  1.0000],\n",
      "        [-1.0000,  0.9604, -0.1418,  ..., -0.1266,  1.0000,  1.0000]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "bs = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, x_trn.shape[0], (bs,), generator=g)\n",
    "    xb, yb = x_trn[ix], y_trn[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[xb]  # embed the characters into vectors\n",
    "    emb_cat = emb.view((emb.shape[0], -1))  # concat the vectors\n",
    "    h_pre_act = emb_cat @ W1 + b1  # hidden layer pre-activation\n",
    "    h = torch.tanh(h_pre_act)  # hidden layer\n",
    "    logits = h @ W2 + b2  # output layer\n",
    "    loss = F.cross_entropy(logits, yb)  # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100_000 else 0.01\n",
    "    with torch.no_grad():\n",
    "        for p in parameters:\n",
    "            p -= lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i % 10_000 == 0:\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "        print(h)\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a83c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
