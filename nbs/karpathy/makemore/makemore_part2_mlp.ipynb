{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b7f4b2",
   "metadata": {},
   "source": [
    "Reference paper: [Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A Neural Probabilistic Language Model. J. Mach. Learn. Res. 3, (February 2003), 1137–1155.](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "\n",
    "One main difference in our implementation is that we are working with characters instead of words. The vocabulary size in the Bengio's paper is 17,000 words, whereas we are going to have a vocabulary of 27 characters (26 characters and the `<.>` special character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e58f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac1d8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"./names.txt\", \"r\").read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f90fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8968805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "s2i = {s: i+1 for i, s in enumerate(chars)}\n",
    "s2i[\".\"] = 0\n",
    "i2s = {i: s for s, i in s2i.items()}\n",
    "print(i2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70694de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "block_size = 3  # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + \".\":\n",
    "        ix = s2i[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(\"\".join(i2s[i] for i in context), \"--->\", i2s[ix])\n",
    "        context = context[1:] + [ix]  # crop and append\n",
    "        \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1912235e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb8ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))  # embeddig matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a923d",
   "metadata": {},
   "source": [
    "We can index this embedding matrix directly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4a5f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4756,  0.6766])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa1316",
   "metadata": {},
   "source": [
    "Or using a multiplication between a OHE vector and the embedding matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82961a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4756,  0.6766])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = F.one_hot(torch.tensor(5), 27).float()  # F.one_hot() return int64 dtype tensors\n",
    "ohe @ C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e30cfbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape  # bs, ctx len, emb lengh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fdf08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 100))  # (ctx len x emb length), hidden dim\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f748283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would like to perform (emb @ W1) + b1, but the dims don't match\n",
    "# ---------------------------------------------------------------------------\n",
    "# RuntimeError                              Traceback (most recent call last)\n",
    "# Cell In[31], line 1\n",
    "# ----> 1 (emb @ W1) + b1\n",
    "\n",
    "# RuntimeError: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9a8a4",
   "metadata": {},
   "source": [
    "To solve this problem, we want to multiply between a matrix of shape `(32, 6)` and one of shape `(6, 100)`. One way to achieve this issue is to use `torch.cat()` and \"manually\" concatenate the embedding representation for each token in the input context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1e4ea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.3 µs ± 205 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "(torch.cat((emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]), 1) @ W1) + b1  # we need to concatenate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925904be",
   "metadata": {},
   "source": [
    "The issue with this approach is that it won't automatically handle increasing/decreasing the context length. \n",
    "\n",
    "Alternatively, we could use `torch unbind()`. This command will return a tuple of all slices along a given dimension, already without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02aba6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.unbind(emb, 1)\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fda8ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 µs ± 78.8 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "(torch.cat(torch.unbind(emb, 1), 1) @ W1) + b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545d9e3",
   "metadata": {},
   "source": [
    "One downside of using `torch.cat()` is that we are creating an entirely new tensor in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32411da",
   "metadata": {},
   "source": [
    "An even more efficient way of doing this is to use the `.view()` method. In each tensor, there is an undelying storage, that we can access with `Tensor.storage()`, which contains all of the numbers in a tensor as a 1-dimensional vector. This in how the tensor is represented in memory ― a 1D vector. When we call `.view()`, we are manipulating some attributes (i.e., offset, stride, and shape) of that tensor, that dictate how this 1D sequence is interpreted to be as a N-dimensional tensor. When we use `.view()`, no memory is being moved, copied, or created. The storage stays the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ce07db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.01 µs ± 69 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "(emb.view((32, -1)) @ W1) + b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8041a5",
   "metadata": {},
   "source": [
    "Another approach, more readable and also performant is to use `.reshape()`. As [this StackOverflow answer](https://stackoverflow.com/a/54507446/2092449) very clearly explains, the difference between `.view()` and `.reshape()` is that the latter might return a new tensor that may be a view of the original tensor, or it may be a new tensor altogether. So, if you just want to reshape tensors, use `.reshape()`. If you're also concerned about memory usage and want to ensure that the two tensors share the same data, use `.view()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "617c022d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.99 µs ± 54.3 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "(emb.reshape((32, -1)) @ W1) + b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cec66a",
   "metadata": {},
   "source": [
    "I suggest to use `view()` whenever possible, for performance reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbfe1953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9872,  0.9964, -1.0000,  ..., -0.9986,  0.2636, -0.9999],\n",
       "        [ 0.0741,  0.9828, -0.9978,  ..., -0.4900, -0.9942, -0.9993],\n",
       "        [ 0.8229, -0.9995,  0.2666,  ..., -0.9851, -0.9740,  0.9977],\n",
       "        ...,\n",
       "        [-0.0168, -0.9989,  0.9989,  ..., -0.8979, -0.7624,  0.9997],\n",
       "        [-0.6276,  0.8327, -0.9993,  ..., -0.9102, -0.8032, -0.9606],\n",
       "        [-0.7889,  1.0000, -0.9999,  ...,  0.4892,  0.6764, -1.0000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.view(emb.shape[0], -1) @ W1 + b1)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9995a9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38380533",
   "metadata": {},
   "source": [
    "One thing to pay attention to, is the addition operation. The resulting matrix generated by multiplying `emb` (reshaped) and `W1` is of size `(32, 100)`, whilst the bias vector has size `(100)`.\n",
    "\n",
    "```\n",
    "32, 100 --> 32, 100\n",
    "    100 -->  1, 100\n",
    "```\n",
    "\n",
    "So the same bias vector will be copied to all the rows of the (`emb.view(emb.shape[0], -1) @ W1`) matrix. Meaning that element `b[0, 0]` will be added to each element of the row `x[0, :]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a439b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
